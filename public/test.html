<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Test Page</title>
</head>
<body>
    <h1>Semantic Search Test Page</h1>
    <p>
        The quick brown fox jumps over the lazy dog. This is a classic sentence used for typography samples.
        The history of the transformer model architecture is fascinating. It was introduced in the paper "Attention Is All You Need" by Vaswani et al. in 2017.
        This new architecture revolutionized natural language processing tasks.
        Transformers are based on the self-attention mechanism, which allows them to weigh the importance of different words in the input sequence.
        This is a key difference from previous models like recurrent neural networks (RNNs) and convolutional neural networks (CNNs).
    </p>
    <img src="icons/icon128.png" alt="An icon representing the extension.">
    <p>
        There are many different types of transformers. BERT, GPT, and T5 are some of the most well-known.
        BERT is a bidirectional model that is pre-trained on a large corpus of text.
        GPT is a unidirectional model that is also pre-trained on a large corpus of text.
        T5 is a text-to-text transfer transformer that can be used for a variety of tasks.
    </p>
    <button aria-label="A button with an ARIA label for testing purposes.">Click me</button>
    <p>
        The sun is a star. It is the center of our solar system.
        The Earth revolves around the sun. The moon revolves around the Earth.
        There are eight planets in our solar system. Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.
        Jupiter is the largest planet in our solar system.
    </p>
    <img src="icons/icon48.png" alt="A smaller icon.">

    <script src="content.js"></script>
</body>
</html>
